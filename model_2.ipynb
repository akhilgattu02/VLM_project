{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1e7bed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.7.0)\n",
      "Collecting torch\n",
      "  Downloading torch-2.9.1-cp313-none-macosx_11_0_arm64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: torchvision in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.22.0)\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.24.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (80.3.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torchvision) (2.2.5)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torchvision) (11.2.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading torch-2.9.1-cp313-none-macosx_11_0_arm64.whl (74.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.5/74.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m  \u001b[33m0:00:21\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.24.1-cp313-cp313-macosx_12_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch, torchvision\n",
      "\u001b[2K  Attempting uninstall: torch\n",
      "\u001b[2K    Found existing installation: torch 2.7.0\n",
      "\u001b[2K    Uninstalling torch-2.7.0:━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled torch-2.7.0━━\u001b[0m \u001b[32m0/2\u001b[0m [torch]\n",
      "\u001b[2K  Attempting uninstall: torchvision━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [torch]\n",
      "\u001b[2K    Found existing installation: torchvision 0.22.032m0/2\u001b[0m [torch]\n",
      "\u001b[2K    Uninstalling torchvision-0.22.0:━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [torch]\n",
      "\u001b[2K      Successfully uninstalled torchvision-0.22.0\u001b[32m0/2\u001b[0m [torch]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [torchvision]\u001b[0m [torchvision]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchaudio 2.7.0 requires torch==2.7.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.9.1 torchvision-0.24.1\n",
      "Collecting segmentation-models-pytorch\n",
      "  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting timm\n",
      "  Downloading timm-1.0.24-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from segmentation-models-pytorch) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from segmentation-models-pytorch) (2.2.5)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from segmentation-models-pytorch) (11.2.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from segmentation-models-pytorch) (0.5.3)\n",
      "Requirement already satisfied: torch>=1.8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from segmentation-models-pytorch) (2.9.1)\n",
      "Requirement already satisfied: torchvision>=0.9 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from segmentation-models-pytorch) (0.24.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from segmentation-models-pytorch) (4.67.1)\n",
      "Requirement already satisfied: pyyaml in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from timm) (6.0.2)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2025.3.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/akhilgattu/Library/Python/3.13/lib/python/site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (25.0)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from huggingface-hub>=0.24->segmentation-models-pytorch) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.8->segmentation-models-pytorch) (80.3.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.8->segmentation-models-pytorch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.8->segmentation-models-pytorch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch>=1.8->segmentation-models-pytorch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sympy>=1.13.3->torch>=1.8->segmentation-models-pytorch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2->torch>=1.8->segmentation-models-pytorch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests->huggingface-hub>=0.24->segmentation-models-pytorch) (2025.4.26)\n",
      "Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n",
      "Downloading timm-1.0.24-py3-none-any.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: timm, segmentation-models-pytorch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [segmentation-models-pytorch]\n",
      "\u001b[1A\u001b[2KSuccessfully installed segmentation-models-pytorch-0.5.0 timm-1.0.24\n",
      "Requirement already satisfied: albumentations in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.0.8)\n",
      "Requirement already satisfied: opencv-python in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: numpy>=1.24.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from albumentations) (2.2.5)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from albumentations) (1.15.2)\n",
      "Requirement already satisfied: PyYAML in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from albumentations) (6.0.2)\n",
      "Requirement already satisfied: pydantic>=2.9.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from albumentations) (2.12.5)\n",
      "Requirement already satisfied: albucore==0.0.24 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from albumentations) (0.0.24)\n",
      "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from albumentations) (4.10.0.84)\n",
      "Requirement already satisfied: stringzilla>=3.10.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from albucore==0.0.24->albumentations) (4.6.0)\n",
      "Requirement already satisfied: simsimd>=5.9.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from albucore==0.0.24->albumentations) (6.5.12)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic>=2.9.2->albumentations) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic>=2.9.2->albumentations) (0.4.2)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.2.5)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.4.1-cp313-cp313-macosx_14_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.10.1)\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.8-cp313-cp313-macosx_11_0_arm64.whl.metadata (52 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/akhilgattu/Library/Python/3.13/lib/python/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/akhilgattu/Library/Python/3.13/lib/python/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/akhilgattu/Library/Python/3.13/lib/python/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading numpy-2.4.1-cp313-cp313-macosx_14_0_arm64.whl (5.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.8-cp313-cp313-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy, matplotlib\n",
      "\u001b[2K  Attempting uninstall: numpy\n",
      "\u001b[2K    Found existing installation: numpy 2.2.5\n",
      "\u001b[2K    Uninstalling numpy-2.2.5:\n",
      "\u001b[2K      Successfully uninstalled numpy-2.2.5\n",
      "\u001b[2K  Attempting uninstall: matplotlib━━━━━━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [numpy]\n",
      "\u001b[2K    Found existing installation: matplotlib 3.10.1[32m0/2\u001b[0m [numpy]\n",
      "\u001b[2K    Uninstalling matplotlib-3.10.1:━━━━━━━━━\u001b[0m \u001b[32m0/2\u001b[0m [numpy]\n",
      "\u001b[2K      Successfully uninstalled matplotlib-3.10.1━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1/2\u001b[0m [matplotlib]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [matplotlib]2\u001b[0m [matplotlib]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
      "streamlit 1.22.0 requires protobuf<4,>=3.12, but you have protobuf 6.32.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed matplotlib-3.10.8 numpy-2.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torch torchvision\n",
    "!pip install -U segmentation-models-pytorch timm\n",
    "!pip install -U albumentations opencv-python\n",
    "!pip install -U numpy tqdm matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba8f3144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing TRAIN split...\n",
      "Found 54 images in /Users/akhilgattu/Desktop/VLM_project/A. Segmentation/1. Original Images/a. Training Set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing a. Training Set: 100%|██████████| 54/54 [00:03<00:00, 14.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing VAL split (using IDRiD test folder as val)...\n",
      "Found 27 images in /Users/akhilgattu/Desktop/VLM_project/A. Segmentation/1. Original Images/b. Testing Set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing b. Testing Set: 100%|██████████| 27/27 [00:01<00:00, 13.52it/s]\n",
      "/var/folders/61/80bh8zbs3rg_9w2k2zqhghd00000gn/T/ipykernel_2730/661149791.py:196: UserWarning: Argument(s) 'value, mask_value' are not valid for transform ShiftScaleRotate\n",
      "  A.ShiftScaleRotate(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | TrainLoss=1.0481 | ValLoss=1.0577 | ValDice(no-bg)=0.0003\n",
      "Saved BEST model: checkpoints/unetpp_effb3_idrid_5class.pth (dice=0.0003)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 02 | TrainLoss=0.8185 | ValLoss=0.7461 | ValDice(no-bg)=0.0029\n",
      "Saved BEST model: checkpoints/unetpp_effb3_idrid_5class.pth (dice=0.0029)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 03 | TrainLoss=0.6836 | ValLoss=0.6323 | ValDice(no-bg)=0.0892\n",
      "Saved BEST model: checkpoints/unetpp_effb3_idrid_5class.pth (dice=0.0892)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 04 | TrainLoss=0.6053 | ValLoss=0.5850 | ValDice(no-bg)=0.3872\n",
      "Saved BEST model: checkpoints/unetpp_effb3_idrid_5class.pth (dice=0.3872)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 05 | TrainLoss=0.5643 | ValLoss=0.5568 | ValDice(no-bg)=0.3767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 06 | TrainLoss=0.5429 | ValLoss=0.5411 | ValDice(no-bg)=0.2771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 07 | TrainLoss=0.5315 | ValLoss=0.5290 | ValDice(no-bg)=0.4299\n",
      "Saved BEST model: checkpoints/unetpp_effb3_idrid_5class.pth (dice=0.4299)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 08 | TrainLoss=0.5251 | ValLoss=0.5243 | ValDice(no-bg)=0.8324\n",
      "Saved BEST model: checkpoints/unetpp_effb3_idrid_5class.pth (dice=0.8324)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 09 | TrainLoss=0.5209 | ValLoss=0.5201 | ValDice(no-bg)=1.0000\n",
      "Saved BEST model: checkpoints/unetpp_effb3_idrid_5class.pth (dice=1.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | TrainLoss=0.5183 | ValLoss=0.5173 | ValDice(no-bg)=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 | TrainLoss=0.5163 | ValLoss=0.5151 | ValDice(no-bg)=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 | TrainLoss=0.5147 | ValLoss=0.5136 | ValDice(no-bg)=1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 376\u001b[39m\n\u001b[32m    373\u001b[39m     prepare_split(TEST_IMG_DIR, TEST_GT_DIR, OUT_VAL_IMG, OUT_VAL_MSK)\n\u001b[32m    375\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 327\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    325\u001b[39m logits = model(imgs)\n\u001b[32m    326\u001b[39m loss = criterion(logits, masks)\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m optimizer.step()\n\u001b[32m    330\u001b[39m tr_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 0) SET YOUR BASE DIR\n",
    "# =========================\n",
    "BASE_DIR = \"/Users/akhilgattu/Desktop/VLM_project/\"  # change if needed\n",
    "\n",
    "SEG_DIR = os.path.join(BASE_DIR, \"A. Segmentation\")\n",
    "\n",
    "TRAIN_IMG_DIR = os.path.join(SEG_DIR, \"1. Original Images\", \"a. Training Set\")\n",
    "TEST_IMG_DIR  = os.path.join(SEG_DIR, \"1. Original Images\", \"b. Testing Set\")\n",
    "\n",
    "TRAIN_GT_DIR = os.path.join(SEG_DIR, \"2. All Segmentation Groundtruths\", \"a. Training Set\")\n",
    "TEST_GT_DIR  = os.path.join(SEG_DIR, \"2. All Segmentation Groundtruths\", \"b. Testing Set\")\n",
    "\n",
    "# output prepared dataset\n",
    "OUT_DIR = \"data_idrid_multiclass\"\n",
    "OUT_TRAIN_IMG = os.path.join(OUT_DIR, \"train\", \"images\")\n",
    "OUT_TRAIN_MSK = os.path.join(OUT_DIR, \"train\", \"masks\")\n",
    "OUT_VAL_IMG   = os.path.join(OUT_DIR, \"val\", \"images\")\n",
    "OUT_VAL_MSK   = os.path.join(OUT_DIR, \"val\", \"masks\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 1) CLASS MAP (IDRiD)\n",
    "# =========================\n",
    "CLASS_TO_ID = {\n",
    "    \"MA\": 1,\n",
    "    \"HE\": 2,\n",
    "    \"EX\": 3,\n",
    "    \"SE\": 4,\n",
    "    \"OD\": 5,\n",
    "}\n",
    "NUM_CLASSES = 6  # including background 0\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 2) HELPERS\n",
    "# =========================\n",
    "def ensure_dirs():\n",
    "    os.makedirs(OUT_TRAIN_IMG, exist_ok=True)\n",
    "    os.makedirs(OUT_TRAIN_MSK, exist_ok=True)\n",
    "    os.makedirs(OUT_VAL_IMG, exist_ok=True)\n",
    "    os.makedirs(OUT_VAL_MSK, exist_ok=True)\n",
    "\n",
    "\n",
    "def list_images(folder):\n",
    "    exts = [\"*.jpg\", \"*.png\", \"*.jpeg\", \"*.tif\", \"*.tiff\"]\n",
    "    files = []\n",
    "    for e in exts:\n",
    "        files += glob(os.path.join(folder, e))\n",
    "    files.sort()\n",
    "    return files\n",
    "\n",
    "\n",
    "def find_mask(gt_root, base_name, lesion_code):\n",
    "    \"\"\"\n",
    "    IDRiD GT names look like:\n",
    "      IDRiD_01_MA.tif\n",
    "      IDRiD_01_HE.tif\n",
    "      IDRiD_01_EX.tif\n",
    "      IDRiD_01_SE.tif\n",
    "      IDRiD_01_OD.tif\n",
    "\n",
    "    base_name: \"IDRiD_01\"\n",
    "    lesion_code: \"MA\", \"HE\", \"EX\", \"SE\", \"OD\"\n",
    "    \"\"\"\n",
    "    patterns = [\n",
    "        os.path.join(gt_root, f\"{base_name}_{lesion_code}.tif\"),\n",
    "        os.path.join(gt_root, f\"{base_name}_{lesion_code}.png\"),\n",
    "        os.path.join(gt_root, f\"{base_name}_{lesion_code}.jpg\"),\n",
    "        os.path.join(gt_root, f\"{base_name}_{lesion_code}.jpeg\"),\n",
    "        os.path.join(gt_root, f\"{base_name}_{lesion_code}.tiff\"),\n",
    "    ]\n",
    "    for p in patterns:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "\n",
    "def build_multiclass_mask(image_shape_hw, gt_root, base_name):\n",
    "    \"\"\"\n",
    "    Merge all binary lesion masks into ONE multiclass mask.\n",
    "    Priority order (if overlap happens):\n",
    "       MA > HE > EX > SE > OD\n",
    "    (you can change it; overlaps are usually minimal)\n",
    "    \"\"\"\n",
    "    H, W = image_shape_hw\n",
    "    mask = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "    priority = [\"OD\", \"SE\", \"EX\", \"HE\", \"MA\"]  # low->high, so MA overwrites others\n",
    "\n",
    "    for lesion in priority:\n",
    "        mp = find_mask(gt_root, base_name, lesion)\n",
    "        if mp is None:\n",
    "            continue\n",
    "\n",
    "        m = cv2.imread(mp, cv2.IMREAD_GRAYSCALE)\n",
    "        if m is None:\n",
    "            continue\n",
    "\n",
    "        # binarize (IDRiD masks are 0/255 typically)\n",
    "        m_bin = (m > 0).astype(np.uint8)\n",
    "        cls_id = CLASS_TO_ID[lesion]\n",
    "\n",
    "        # assign this class where lesion exists\n",
    "        mask[m_bin == 1] = cls_id\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def prepare_split(images_dir, gts_dir, out_img_dir, out_msk_dir):\n",
    "    img_paths = list_images(images_dir)\n",
    "    if len(img_paths) == 0:\n",
    "        raise RuntimeError(f\"No images found in: {images_dir}\")\n",
    "\n",
    "    print(f\"Found {len(img_paths)} images in {images_dir}\")\n",
    "\n",
    "    for ip in tqdm(img_paths, desc=f\"Preparing {os.path.basename(images_dir)}\"):\n",
    "        img = cv2.imread(ip, cv2.IMREAD_COLOR)\n",
    "        if img is None:\n",
    "            raise FileNotFoundError(f\"Cannot read image: {ip}\")\n",
    "\n",
    "        H, W = img.shape[:2]\n",
    "        base = os.path.splitext(os.path.basename(ip))[0]  # e.g. IDRiD_01\n",
    "\n",
    "        multiclass_mask = build_multiclass_mask((H, W), gts_dir, base)\n",
    "\n",
    "        # save image as PNG/JPG (keep original extension if you want)\n",
    "        out_img_path = os.path.join(out_img_dir, os.path.basename(ip))\n",
    "        out_msk_path = os.path.join(out_msk_dir, f\"{base}.tif\")  # single-channel PNG\n",
    "\n",
    "        cv2.imwrite(out_img_path, img)\n",
    "        cv2.imwrite(out_msk_path, multiclass_mask)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) DATASET + AUGS\n",
    "# =========================\n",
    "class MultiClassSegDataset(Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, transform=None):\n",
    "        self.images = list_images(images_dir)\n",
    "        self.masks = []\n",
    "        for ip in self.images:\n",
    "            base = os.path.splitext(os.path.basename(ip))[0]\n",
    "            mp = os.path.join(masks_dir, f\"{base}.tif\")\n",
    "            self.masks.append(mp)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ip = self.images[idx]\n",
    "        mp = self.masks[idx]\n",
    "\n",
    "        img = cv2.imread(ip, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = cv2.imread(mp, cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            raise FileNotFoundError(f\"Missing mask: {mp}\")\n",
    "\n",
    "        if self.transform:\n",
    "            out = self.transform(image=img, mask=mask)\n",
    "            img = out[\"image\"]\n",
    "            mask = out[\"mask\"]\n",
    "\n",
    "        mask = torch.as_tensor(mask, dtype=torch.long)\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "def get_train_tfms(sz=512):\n",
    "    return A.Compose([\n",
    "        A.Resize(sz, sz),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.CLAHE(p=0.3),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.3),\n",
    "        A.ShiftScaleRotate(\n",
    "            shift_limit=0.05, scale_limit=0.10, rotate_limit=20,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            value=0, mask_value=0, p=0.5\n",
    "        ),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                    std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_val_tfms(sz=512):\n",
    "    return A.Compose([\n",
    "        A.Resize(sz, sz),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                    std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 4) LOSSES (Dice + Focal)\n",
    "# =========================\n",
    "class DiceLossMulticlass(nn.Module):\n",
    "    def __init__(self, num_classes, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        targets_1h = F.one_hot(targets, num_classes=self.num_classes).permute(0, 3, 1, 2).float()\n",
    "\n",
    "        dims = (0, 2, 3)\n",
    "        intersection = torch.sum(probs * targets_1h, dims)\n",
    "        union = torch.sum(probs, dims) + torch.sum(targets_1h, dims)\n",
    "\n",
    "        dice = (2.0 * intersection + self.smooth) / (union + self.smooth)\n",
    "        return 1.0 - dice.mean()\n",
    "\n",
    "\n",
    "class FocalLossMulticlass(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=None):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        ce = F.cross_entropy(logits, targets, weight=self.alpha, reduction=\"none\")\n",
    "        pt = torch.exp(-ce)\n",
    "        loss = ((1 - pt) ** self.gamma) * ce\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "class DiceFocalLoss(nn.Module):\n",
    "    def __init__(self, num_classes, dice_w=0.6, focal_w=0.4, gamma=2.0, alpha=None):\n",
    "        super().__init__()\n",
    "        self.dice = DiceLossMulticlass(num_classes)\n",
    "        self.focal = FocalLossMulticlass(gamma=gamma, alpha=alpha)\n",
    "        self.dice_w = dice_w\n",
    "        self.focal_w = focal_w\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        return self.dice_w * self.dice(logits, targets) + self.focal_w * self.focal(logits, targets)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def mean_dice_no_bg(logits, targets, num_classes):\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    dices = []\n",
    "    for c in range(1, num_classes):\n",
    "        p = (preds == c).float()\n",
    "        t = (targets == c).float()\n",
    "        inter = (p * t).sum()\n",
    "        denom = p.sum() + t.sum()\n",
    "        d = (2 * inter + 1.0) / (denom + 1.0)\n",
    "        dices.append(d)\n",
    "    return torch.stack(dices).mean().item()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) TRAINING\n",
    "# =========================\n",
    "def train():\n",
    "    device = torch.device(\"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    img_size = 512\n",
    "    batch_size = 4\n",
    "    epochs = 35\n",
    "    lr = 3e-4\n",
    "\n",
    "    train_ds = MultiClassSegDataset(\n",
    "        os.path.join(OUT_DIR, \"train\", \"images\"),\n",
    "        os.path.join(OUT_DIR, \"train\", \"masks\"),\n",
    "        transform=get_train_tfms(img_size),\n",
    "    )\n",
    "    val_ds = MultiClassSegDataset(\n",
    "        os.path.join(OUT_DIR, \"val\", \"images\"),\n",
    "        os.path.join(OUT_DIR, \"val\", \"masks\"),\n",
    "        transform=get_val_tfms(img_size),\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    model = smp.UnetPlusPlus(\n",
    "        encoder_name=\"timm-efficientnet-b3\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        in_channels=3,\n",
    "        classes=NUM_CLASSES,\n",
    "        activation=None\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = DiceFocalLoss(NUM_CLASSES, dice_w=0.6, focal_w=0.4, gamma=2.0)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    best = -1e9\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        tr_loss = 0.0\n",
    "\n",
    "        for imgs, masks in tqdm(train_loader, desc=f\"Train {ep}/{epochs}\", leave=False):\n",
    "            imgs = imgs.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            logits = model(imgs)\n",
    "            loss = criterion(logits, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "        tr_loss /= max(1, len(train_loader))\n",
    "\n",
    "        # val\n",
    "        model.eval()\n",
    "        va_loss = 0.0\n",
    "        va_dice = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in tqdm(val_loader, desc=f\"Val {ep}/{epochs}\", leave=False):\n",
    "                imgs = imgs.to(device)\n",
    "                masks = masks.to(device)\n",
    "\n",
    "                logits = model(imgs)\n",
    "                loss = criterion(logits, masks)\n",
    "                va_loss += loss.item()\n",
    "                va_dice += mean_dice_no_bg(logits, masks, NUM_CLASSES)\n",
    "\n",
    "        va_loss /= max(1, len(val_loader))\n",
    "        va_dice /= max(1, len(val_loader))\n",
    "\n",
    "        print(f\"Epoch {ep:02d} | TrainLoss={tr_loss:.4f} | ValLoss={va_loss:.4f} | ValDice(no-bg)={va_dice:.4f}\")\n",
    "\n",
    "        if va_dice > best:\n",
    "            best = va_dice\n",
    "            torch.save(model.state_dict(), \"checkpoints/unetpp_effb3_idrid_5class.pth\")\n",
    "            print(f\"Saved BEST model: checkpoints/unetpp_effb3_idrid_5class.pth (dice={best:.4f})\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# RUN\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    ensure_dirs()\n",
    "\n",
    "    # prepare dataset only if not prepared already\n",
    "    if len(os.listdir(OUT_TRAIN_MSK)) == 0:\n",
    "        print(\"\\nPreparing TRAIN split...\")\n",
    "        prepare_split(TRAIN_IMG_DIR, TRAIN_GT_DIR, OUT_TRAIN_IMG, OUT_TRAIN_MSK)\n",
    "\n",
    "    if len(os.listdir(OUT_VAL_MSK)) == 0:\n",
    "        print(\"\\nPreparing VAL split (using IDRiD test folder as val)...\")\n",
    "        prepare_split(TEST_IMG_DIR, TEST_GT_DIR, OUT_VAL_IMG, OUT_VAL_MSK)\n",
    "\n",
    "    print(\"\\nStarting training...\")\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2c686ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/akhilgattu/Desktop/VLM_project\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3efbc67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Found 27 images.\n",
      "\n",
      "Saved results in: visual_results\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# CONFIG\n",
    "# --------------------------\n",
    "CKPT_PATH = \"/Users/akhilgattu/Desktop/VLM_project/checkpoints/unetpp_effb3_idrid_5class.pth\"\n",
    "\n",
    "# Put your test images here (any folder)\n",
    "# Example: use official IDRiD test images\n",
    "IMG_DIR = \"/Users/akhilgattu/Desktop/VLM_project/A. Segmentation/1. Original Images/b. Testing Set\"\n",
    "\n",
    "OUT_DIR = \"visual_results\"\n",
    "IMG_SIZE = 512\n",
    "\n",
    "NUM_CLASSES = 6  # 0..5\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# CLASS COLORS (BGR)\n",
    "# 0=BG, 1=MA, 2=HE, 3=EX, 4=SE, 5=OD\n",
    "# --------------------------\n",
    "COLORS = {\n",
    "    0: (0, 0, 0),         # BG - black\n",
    "    1: (0, 0, 255),       # MA - red\n",
    "    2: (0, 165, 255),     # HE - orange\n",
    "    3: (0, 255, 255),     # EX - yellow\n",
    "    4: (255, 0, 255),     # SE - magenta\n",
    "    5: (255, 255, 0),     # OD - cyan\n",
    "}\n",
    "\n",
    "CLASS_NAMES = {\n",
    "    0: \"BG\",\n",
    "    1: \"MA\",\n",
    "    2: \"HE\",\n",
    "    3: \"EX\",\n",
    "    4: \"SE\",\n",
    "    5: \"OD\",\n",
    "}\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# TRANSFORM (same as training normalize)\n",
    "# --------------------------\n",
    "def get_tfms(img_size=512):\n",
    "    return A.Compose([\n",
    "        A.Resize(img_size, img_size),\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                    std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# UTILS\n",
    "# --------------------------\n",
    "def list_images(folder):\n",
    "    exts = (\".jpg\", \".jpeg\", \".png\", \".tif\", \".tiff\")\n",
    "    files = [f for f in os.listdir(folder) if f.lower().endswith(exts)]\n",
    "    files.sort()\n",
    "    return [os.path.join(folder, f) for f in files]\n",
    "\n",
    "\n",
    "def colorize_mask(mask_0_5):\n",
    "    \"\"\"mask_0_5: HxW int values 0..5\"\"\"\n",
    "    h, w = mask_0_5.shape\n",
    "    color = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for k, bgr in COLORS.items():\n",
    "        color[mask_0_5 == k] = bgr\n",
    "    return color\n",
    "\n",
    "\n",
    "def overlay(image_bgr, color_mask_bgr, alpha=0.45):\n",
    "    return cv2.addWeighted(image_bgr, 1 - alpha, color_mask_bgr, alpha, 0)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# MAIN\n",
    "# --------------------------\n",
    "def main():\n",
    "    os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # Load model\n",
    "    model = smp.UnetPlusPlus(\n",
    "        encoder_name=\"timm-efficientnet-b3\",\n",
    "        encoder_weights=None,  # IMPORTANT: weights come from checkpoint\n",
    "        in_channels=3,\n",
    "        classes=NUM_CLASSES,\n",
    "        activation=None\n",
    "    ).to(device)\n",
    "\n",
    "    state = torch.load(CKPT_PATH, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.eval()\n",
    "\n",
    "    tfms = get_tfms(IMG_SIZE)\n",
    "\n",
    "    img_paths = list_images(IMG_DIR)\n",
    "    if len(img_paths) == 0:\n",
    "        raise RuntimeError(f\"No images found in {IMG_DIR}\")\n",
    "\n",
    "    print(f\"Found {len(img_paths)} images.\")\n",
    "\n",
    "    for ip in img_paths:\n",
    "        fname = os.path.splitext(os.path.basename(ip))[0]\n",
    "\n",
    "        # read original image\n",
    "        img_bgr = cv2.imread(ip, cv2.IMREAD_COLOR)\n",
    "        if img_bgr is None:\n",
    "            print(f\"Skipping (cannot read): {ip}\")\n",
    "            continue\n",
    "\n",
    "        img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # transform\n",
    "        out = tfms(image=img_rgb)\n",
    "        x = out[\"image\"].unsqueeze(0).to(device)  # [1,3,H,W]\n",
    "\n",
    "        # predict\n",
    "        with torch.no_grad():\n",
    "            logits = model(x)                      # [1,6,H,W]\n",
    "            pred = torch.argmax(logits, dim=1)     # [1,H,W]\n",
    "            pred_mask = pred.squeeze(0).cpu().numpy().astype(np.uint8)\n",
    "\n",
    "        # colorize + overlay\n",
    "        pred_color = colorize_mask(pred_mask)\n",
    "        pred_overlay = overlay(\n",
    "            cv2.resize(img_bgr, (IMG_SIZE, IMG_SIZE)),\n",
    "            pred_color,\n",
    "            alpha=0.45\n",
    "        )\n",
    "\n",
    "        # save outputs\n",
    "        # 1) raw mask (0..5)\n",
    "        cv2.imwrite(os.path.join(OUT_DIR, f\"{fname}_pred_mask.png\"), pred_mask)\n",
    "\n",
    "        # 2) color mask\n",
    "        cv2.imwrite(os.path.join(OUT_DIR, f\"{fname}_pred_color.png\"), pred_color)\n",
    "\n",
    "        # 3) overlay\n",
    "        cv2.imwrite(os.path.join(OUT_DIR, f\"{fname}_overlay.png\"), pred_overlay)\n",
    "\n",
    "    print(f\"\\nSaved results in: {OUT_DIR}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
